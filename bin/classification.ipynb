{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "\n",
    "Entrenar un modelo de red neuronal artificial profunda para resolver una tarea de clasificación multiclase usando PyTorch.\n",
    "\n",
    "\n",
    "## Descripción de la tarea\n",
    "\n",
    "Vamos a usar el conjunto de datos [cardiovascular_disease.csv](https://www.kaggle.com/datasets/bhadaneeraj/cardio-vascular-disease-detection?select=cardio_train.csv).\n",
    "\n",
    "El modelo debe clasificar si los pacientes tienen o no, enfermedad cardiovascular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronal network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de conjuntos de datos\n",
    "\n",
    "> Entrenamiento\n",
    "> \n",
    "> Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65390, 18)\n",
      "['No Disease' 'Disease']\n",
      "(52312, 18)\n",
      "(13078, 18)\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.read_csv('./../data/filtered_cardiovascular_disease.csv', index_col= 0) # Dataset viejo\n",
    "# df_data = pd.read_csv('./../data/.csv', index_col= 0) # Dataset nuevo\n",
    "df_data['cardio'] = df_data['cardio'].replace({0: 'No Disease', 1: 'Disease'})\n",
    "scaler = StandardScaler()\n",
    "df_data[['age_days', 'age_year', 'height', 'weight', 'bmi', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'age_group', 'pulse', 'pulse_group', 'aphi_group', 'ap_hi', 'ap_lo']] = scaler.fit_transform(df_data[['age_days', 'age_year', 'height', 'weight', 'bmi', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'age_group', 'pulse', 'pulse_group', 'aphi_group', 'ap_hi', 'ap_lo']]) # Estandarizar/Normalizar\n",
    "# df_data = df_data.drop(columns=['age_days']) # Todas las columnas a quitar\n",
    "print(df_data.shape)\n",
    "print(df_data.cardio.unique())\n",
    "train_df, test_df = train_test_split(df_data, test_size=0.2, random_state= 42)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objeto CardioDataset\n",
    "\n",
    "Sobre escribe algunos métodos del objeto Dataset de <torch.utils.data>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_ids = {\n",
    "    'No Disease' : 0,\n",
    "    'Disease' : 1\n",
    "}\n",
    "\n",
    "ids_to_labels = {v: k for v, k in enumerate(labels_to_ids)}\n",
    "\n",
    "class CardioDataset(Dataset):\n",
    "    \"\"\"NER dataset.\"\"\"\n",
    "    def __init__(self, dataframe):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data : Pandas dataframe.\n",
    "        \"\"\"\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # # step 1: get data and label\n",
    "        features = self.data.drop(['cardio'], axis=1).iloc[index]\n",
    "        cardio_labels = self.data.cardio.iloc[index]\n",
    "        # print(cardio_labels)\n",
    "\n",
    "        # step 2: encode labels\n",
    "        encoded_labels = labels_to_ids[cardio_labels]\n",
    "\n",
    "        # step 3: turn everything into PyTorch tensors\n",
    "        pacient_features = torch.as_tensor(features)\n",
    "        label = torch.as_tensor(encoded_labels)\n",
    "        return pacient_features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "train_dataset = CardioDataset(dataframe=train_df)\n",
    "test_dataset = CardioDataset(dataframe=test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de `DataLoader`\n",
    "\n",
    "Definimos un tamaño de lote para que la red ajuste sus pesos cada 128 ejemplos (`batch_size = 128`).\n",
    "\n",
    "Creamos dos objetos `DataLoader` para iterar ambos conjuntos de datos por lotes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidad de datos X: torch.Size([128, 17])\n",
      "Dimensionalidad de categorías y: torch.Size([128]), tipo de categorías: torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle= True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle= True)\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(f\"Dimensionalidad de datos X: {X.shape}\")\n",
    "    print(f\"Dimensionalidad de categorías y: {y.shape}, tipo de categorías: {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de `device` para entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device a utilizar: cpu\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Device a utilizar: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de arquitectura de la red neuronal\n",
    "\n",
    "Creamos la clase `RedNeuronal` a partir de la clase base [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). Esta clase será definida de acuerdo a la arquitectura de red que deseamos entrenar.\n",
    "\n",
    "Debemos definir:\n",
    "- Capas\n",
    "- Nodos por capa\n",
    "- Función de activación\n",
    "- Procesamiento hacia adelante de la red (`forward`)\n",
    "\n",
    "Al final indicamos que la red debe crearse en el `device` definido anteriormente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RedNeuronal(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=17, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (13): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define modelo\n",
    "class RedNeuronal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(17, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.double()\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = RedNeuronal().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de función de pérdida y optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de método de entrenamiento (`train`)\n",
    "\n",
    "Esto incluye procesar los datos del `dataloader` lote por lote \"hacia delante\" y calcular el error de predicción de la red, esto es, el valor de la función de pérdida. Luego debemos propagar el error de predicción \"hacia atrás\" de la red para ajustar los pesos de los nodos. En cada iteración, la red ajustará los pesos para predecir cada vez mejor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.double().view(-1, 1))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"Valor de función de pérdida: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de método de prueba (`test`)\n",
    "\n",
    "Esto incluye procesar los datos de prueba lote por lote, esto es, hacer que la red previamente entrenada realice predicciones sobre estos datos y acumular tanto el error (valor de la función de perdida) como los aciertos de la red (valor de exactitud o `Accuracy`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, threshold):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y.double().view(-1, 1)).item()\n",
    "            predictions = (pred > threshold).float()\n",
    "            correct += (predictions.view(-1) == y.float()).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Fase de prueba: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, Probability Threshold: {threshold} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------\n",
    "## Entrenamiento de la red neuronal\n",
    "\n",
    "Ejecutamos los métodos de entrenamiento (`train`) y prueba (`test`) por un número de épocas (veces que la red entrena usando todos los datos disponibles).\n",
    "\n",
    "El `train` imprimirá el valor de pérdida por cada 10 lotes. El método `test` imprimirá el valor de pérdida y la exactitud de predicción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Valor de función de pérdida: 0.644960  [   64/52312]\n",
      "Valor de función de pérdida: 0.637332  [  704/52312]\n",
      "Valor de función de pérdida: 0.664951  [ 1344/52312]\n",
      "Valor de función de pérdida: 0.647771  [ 1984/52312]\n",
      "Valor de función de pérdida: 0.645988  [ 2624/52312]\n",
      "Valor de función de pérdida: 0.617003  [ 3264/52312]\n",
      "Valor de función de pérdida: 0.621234  [ 3904/52312]\n",
      "Valor de función de pérdida: 0.722292  [ 4544/52312]\n",
      "Valor de función de pérdida: 0.642768  [ 5184/52312]\n",
      "Valor de función de pérdida: 0.637257  [ 5824/52312]\n",
      "Valor de función de pérdida: 0.624610  [ 6464/52312]\n",
      "Valor de función de pérdida: 0.679570  [ 7104/52312]\n",
      "Valor de función de pérdida: 0.643154  [ 7744/52312]\n",
      "Valor de función de pérdida: 0.641688  [ 8384/52312]\n",
      "Valor de función de pérdida: 0.647971  [ 9024/52312]\n",
      "Valor de función de pérdida: 0.680272  [ 9664/52312]\n",
      "Valor de función de pérdida: 0.631723  [10304/52312]\n",
      "Valor de función de pérdida: 0.634397  [10944/52312]\n",
      "Valor de función de pérdida: 0.631984  [11584/52312]\n",
      "Valor de función de pérdida: 0.603106  [12224/52312]\n",
      "Valor de función de pérdida: 0.612733  [12864/52312]\n",
      "Valor de función de pérdida: 0.645839  [13504/52312]\n",
      "Valor de función de pérdida: 0.647116  [14144/52312]\n",
      "Valor de función de pérdida: 0.637073  [14784/52312]\n",
      "Valor de función de pérdida: 0.626423  [15424/52312]\n",
      "Valor de función de pérdida: 0.632297  [16064/52312]\n",
      "Valor de función de pérdida: 0.703740  [16704/52312]\n",
      "Valor de función de pérdida: 0.623450  [17344/52312]\n",
      "Valor de función de pérdida: 0.598978  [17984/52312]\n",
      "Valor de función de pérdida: 0.619500  [18624/52312]\n",
      "Valor de función de pérdida: 0.589145  [19264/52312]\n",
      "Valor de función de pérdida: 0.691345  [19904/52312]\n",
      "Valor de función de pérdida: 0.673300  [20544/52312]\n",
      "Valor de función de pérdida: 0.587551  [21184/52312]\n",
      "Valor de función de pérdida: 0.649534  [21824/52312]\n",
      "Valor de función de pérdida: 0.598234  [22464/52312]\n",
      "Valor de función de pérdida: 0.641316  [23104/52312]\n",
      "Valor de función de pérdida: 0.614921  [23744/52312]\n",
      "Valor de función de pérdida: 0.702481  [24384/52312]\n",
      "Valor de función de pérdida: 0.613533  [25024/52312]\n",
      "Valor de función de pérdida: 0.609483  [25664/52312]\n",
      "Valor de función de pérdida: 0.642100  [26304/52312]\n",
      "Valor de función de pérdida: 0.638025  [26944/52312]\n",
      "Valor de función de pérdida: 0.638189  [27584/52312]\n",
      "Valor de función de pérdida: 0.581420  [28224/52312]\n",
      "Valor de función de pérdida: 0.577386  [28864/52312]\n",
      "Valor de función de pérdida: 0.597373  [29504/52312]\n",
      "Valor de función de pérdida: 0.651731  [30144/52312]\n",
      "Valor de función de pérdida: 0.568475  [30784/52312]\n",
      "Valor de función de pérdida: 0.623927  [31424/52312]\n",
      "Valor de función de pérdida: 0.599266  [32064/52312]\n",
      "Valor de función de pérdida: 0.578249  [32704/52312]\n",
      "Valor de función de pérdida: 0.649564  [33344/52312]\n",
      "Valor de función de pérdida: 0.622863  [33984/52312]\n",
      "Valor de función de pérdida: 0.611419  [34624/52312]\n",
      "Valor de función de pérdida: 0.604772  [35264/52312]\n",
      "Valor de función de pérdida: 0.602996  [35904/52312]\n",
      "Valor de función de pérdida: 0.581959  [36544/52312]\n",
      "Valor de función de pérdida: 0.666581  [37184/52312]\n",
      "Valor de función de pérdida: 0.578084  [37824/52312]\n",
      "Valor de función de pérdida: 0.681613  [38464/52312]\n",
      "Valor de función de pérdida: 0.675706  [39104/52312]\n",
      "Valor de función de pérdida: 0.585289  [39744/52312]\n",
      "Valor de función de pérdida: 0.565936  [40384/52312]\n",
      "Valor de función de pérdida: 0.627423  [41024/52312]\n",
      "Valor de función de pérdida: 0.602681  [41664/52312]\n",
      "Valor de función de pérdida: 0.572815  [42304/52312]\n",
      "Valor de función de pérdida: 0.627111  [42944/52312]\n",
      "Valor de función de pérdida: 0.581524  [43584/52312]\n",
      "Valor de función de pérdida: 0.610181  [44224/52312]\n",
      "Valor de función de pérdida: 0.626096  [44864/52312]\n",
      "Valor de función de pérdida: 0.633121  [45504/52312]\n",
      "Valor de función de pérdida: 0.573288  [46144/52312]\n",
      "Valor de función de pérdida: 0.648851  [46784/52312]\n",
      "Valor de función de pérdida: 0.620127  [47424/52312]\n",
      "Valor de función de pérdida: 0.588757  [48064/52312]\n",
      "Valor de función de pérdida: 0.586024  [48704/52312]\n",
      "Valor de función de pérdida: 0.621856  [49344/52312]\n",
      "Valor de función de pérdida: 0.618329  [49984/52312]\n",
      "Valor de función de pérdida: 0.592457  [50624/52312]\n",
      "Valor de función de pérdida: 0.576278  [51264/52312]\n",
      "Valor de función de pérdida: 0.589822  [51904/52312]\n",
      "Fase de prueba: \n",
      " Accuracy: 69.4%, Avg loss: 0.593812, Probability Threshold: 0.5 \n",
      "\n",
      "Fase de prueba: \n",
      " Accuracy: 66.8%, Avg loss: 0.593496, Probability Threshold: 0.6 \n",
      "\n",
      "Fase de prueba: \n",
      " Accuracy: 61.4%, Avg loss: 0.593748, Probability Threshold: 0.7 \n",
      "\n",
      "Fase de prueba: \n",
      " Accuracy: 54.2%, Avg loss: 0.593514, Probability Threshold: 0.8 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn, 0.5)\n",
    "    test(test_dataloader, model, loss_fn, 0.6)\n",
    "    test(test_dataloader, model, loss_fn, 0.7)\n",
    "    test(test_dataloader, model, loss_fn, 0.8)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardamos el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./../results/Model73.1_symetric_20epoch.pth\")\n",
    "print(\"Guardamos modelo entrenado en model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "\n",
    "## Fase de inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PepeS\\AppData\\Local\\Temp\\ipykernel_19060\\2675891372.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  trained_model.load_state_dict(torch.load(\"./../results/nn_models/Model73.7_symetric_10epoch.pth\", map_location=torch.device(device)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = RedNeuronal().to(device)\n",
    "trained_model.load_state_dict(torch.load(\"./../results/nn_models/Model73.7_symetric_10epoch.pth\", map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar modelo entrenado\n",
    "\n",
    "Usamos nuestro modelo entrenado en modo de evaluación `model.eval()` y sin ajustes de pesos: `torch.no_grad()`.\n",
    "\n",
    "Imprimimos la categoría predicha por el modelo y la categoría verdadera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataloader, model, threshold):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            pred_probability = pred[0].float().item()\n",
    "            print(pred_probability)\n",
    "            predicted = 1 if pred_probability >= threshold else 0\n",
    "            actual = y.item()\n",
    "            print(f'Predicted: \"{ids_to_labels[predicted]}\", Actual: \"{ids_to_labels[actual]}\"')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5256251692771912\n",
      "Predicted: \"Disease\", Actual: \"Disease\"\n",
      "0.5254349708557129\n",
      "Predicted: \"Disease\", Actual: \"Disease\"\n",
      "0.5261239409446716\n",
      "Predicted: \"Disease\", Actual: \"Disease\"\n",
      "0.5255059599876404\n",
      "Predicted: \"Disease\", Actual: \"No Disease\"\n",
      "0.5254659652709961\n",
      "Predicted: \"Disease\", Actual: \"Disease\"\n"
     ]
    }
   ],
   "source": [
    "predict_dataset = CardioDataset(dataframe=test_df.iloc[:5])\n",
    "predict_dataloader = DataLoader(predict_dataset, batch_size=1, shuffle=False)\n",
    "predict(predict_dataloader, model, 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netective",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
